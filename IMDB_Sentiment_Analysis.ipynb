{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6b44ce39-46f0-446f-814c-c4e67251d04b",
      "metadata": {
        "id": "6b44ce39-46f0-446f-814c-c4e67251d04b"
      },
      "source": [
        "# Contextual Word Sentiment Classification\n",
        "\n",
        "This notebook implements a contextual word sentiment classification model using the IMDb dataset.\n",
        "The goal is to classify individual words as positive, negative, or neutral based on sentence-level sentiment labels,\n",
        "while incorporating the context of neighboring words.\n",
        "\n",
        "\n",
        "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
        "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
        "- Steps taken to alleviate difficulties\n",
        "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
        "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
        "- If you have an idea how this could be extended in an interesting way, describe it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f99fb915-f1a3-4044-a2f5-b63b9e15ab72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f99fb915-f1a3-4044-a2f5-b63b9e15ab72",
        "outputId": "1bf896ab-4cad-4a25-a978-66da33596327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')  # For tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "id": "Zin7rGR3lZJc"
      },
      "id": "Zin7rGR3lZJc",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f33287f5-d864-4b09-b0a0-fe077f9545a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "f33287f5-d864-4b09-b0a0-fe077f9545a3",
        "outputId": "7cb92ee8-dced-43ab-9a99-d2b3f3d11514"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2156980dcd43>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Task 1: Load and preprocess the IMDb dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# IMDb dataset can be loaded from torchtext or manually via pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load IMDb dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Task 1: Load and preprocess the IMDb dataset\n",
        "# IMDb dataset can be loaded from torchtext or manually via pandas\n",
        "from torchtext.datasets import IMDB\n",
        "\n",
        "# Load IMDb dataset\n",
        "train_data, test_data = IMDB(split=('train', 'test'))\n",
        "\n",
        "# Convert the data into lists\n",
        "train_data = [(label, text) for label, text in train_data]\n",
        "test_data = [(label, text) for label, text in test_data]\n",
        "\n",
        "# Combine the datasets for label propagation\n",
        "data = train_data + test_data\n",
        "\n",
        "# Convert to DataFrame for easier processing\n",
        "df = pd.DataFrame(data, columns=['label', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.groupby('label').sample(5)"
      ],
      "metadata": {
        "id": "gfDyCiP0p2ts"
      },
      "id": "gfDyCiP0p2ts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7612cbfc-672d-4ff5-9c4e-9ebebcc5d907",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7612cbfc-672d-4ff5-9c4e-9ebebcc5d907",
        "outputId": "6c809879-75b1-4ebf-884f-af34a8a41909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (757.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext\n",
            "  Downloading https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.17.0) (11.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.6.77)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Collecting torchdata==0.7.1 (from torchtext)\n",
            "  Downloading https://download.pytorch.org/whl/torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.2.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.17.0) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchdata, torchaudio, torchtext\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n"
          ]
        }
      ],
      "source": [
        "## Tip. You can get the dataset from torchtext but the package is old and needs pytorch version 2.2 to work\n",
        "## If you want to use it choose your versions like this:\n",
        "!pip install -U torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121 torchtext\n",
        "# from torchtext.datasets import IMDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84585960-7a22-434c-b1b5-89dcf3229af3",
      "metadata": {
        "id": "84585960-7a22-434c-b1b5-89dcf3229af3"
      },
      "outputs": [],
      "source": [
        "# Task 2: Implement tokenization and label propagation\n",
        "# Implement a function to calculate sentiment scores for each word based on sentence-level labels.\n",
        "# The function should propagate labels to individual words and calculate a soft score for each word.\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Convertit les tags POS de nltk en tags WordNet.\"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Par défaut, on retourne 'NOUN'"
      ],
      "metadata": {
        "id": "-l9qhwEukBs8"
      },
      "id": "-l9qhwEukBs8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "  tokens = word_tokenize(text.lower())\n",
        "  tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "\n",
        "  tokens = [token for token in tokens if token not in stopwords.words('english') and token != 'br']\n",
        "\n",
        "  lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "  pos_tags = nltk.pos_tag(tokens)\n",
        "  tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "df['tokens'] = df['text'].apply(preprocess_text)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1E_6clTUE35o"
      },
      "id": "1E_6clTUE35o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data_preprocessed.csv')"
      ],
      "metadata": {
        "id": "IPy0ORNmLhqr"
      },
      "id": "IPy0ORNmLhqr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6spzYKu9Q3be"
      },
      "id": "6spzYKu9Q3be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eca06a79-1b47-4fde-97a7-eeeec9304293",
      "metadata": {
        "id": "eca06a79-1b47-4fde-97a7-eeeec9304293"
      },
      "outputs": [],
      "source": [
        "# Hint: You can use word_tokenize for tokenization\n",
        "# Hint: You can use a dictionary to store counts of positive and negative labels for each word.\n",
        "\n",
        "# Task 3: Prepare data for contextual learning\n",
        "# Implement a class to create a dataset with context windows.\n",
        "# Each data point should include the word embedding for the target word,\n",
        "# as well as an averaged embedding of the context words in a defined window size.\n",
        "\n",
        "# Use a pre-trained embedding model like GloVe. Download the embeddings and load them into a dictionary.\n",
        "# Example: {\"word\": embedding_vector}\n",
        "\n",
        "# Class signature example:\n",
        "# class WordContextDataset(Dataset):\n",
        "#     def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
        "#         # Your code here\n",
        "#         pass\n",
        "\n",
        "#     def __len__(self):\n",
        "#         # Your code here\n",
        "#         pass\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Your code here\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf77713-74e7-40c7-b50d-5f42dd28466b",
      "metadata": {
        "id": "eaf77713-74e7-40c7-b50d-5f42dd28466b"
      },
      "outputs": [],
      "source": [
        "# Task 4: Define and train the model\n",
        "# Define a neural network for sentiment classification using PyTorch.\n",
        "# The network should take an input vector of concatenated word and context embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29d625da-c09a-443b-8c2c-795e01d750d4",
      "metadata": {
        "id": "29d625da-c09a-443b-8c2c-795e01d750d4"
      },
      "outputs": [],
      "source": [
        "# Example:\n",
        "# class SentimentClassifier(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super(SentimentClassifier, self).__init__()\n",
        "#         # Your code here\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Your code here\n",
        "#         pass\n",
        "\n",
        "# Implement a training loop to train the model on the dataset created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7beed1f0-9b85-4121-801f-b8217996f87f",
      "metadata": {
        "id": "7beed1f0-9b85-4121-801f-b8217996f87f"
      },
      "outputs": [],
      "source": [
        "# Task 5: Evaluate the model\n",
        "# Evaluate the trained model on a validation set.\n",
        "# Use metrics such as precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df935c85-b28c-4b88-9d14-9104095272d9",
      "metadata": {
        "id": "df935c85-b28c-4b88-9d14-9104095272d9"
      },
      "outputs": [],
      "source": [
        "# Example code to evaluate the model:\n",
        "# with torch.no_grad():\n",
        "#     # Predict on validation data and calculate metrics\n",
        "#     pass\n",
        "\n",
        "# Optional: Experiment with hyperparameters or model architecture to improve performance.\n",
        "# Examples: Try different window sizes, embedding dimensions, or additional layers in the model."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}